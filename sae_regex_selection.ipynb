{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────────────────────┬─────────────────────────────────────────────────────┬────────────────────────────────────────────────────────┬──────────┐\n",
      "│ model                               │ release                                             │ repo_id                                                │   n_saes │\n",
      "├─────────────────────────────────────┼─────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┼──────────┤\n",
      "│ gemma-2-27b                         │ gemma-scope-27b-pt-res                              │ google/gemma-scope-27b-pt-res                          │       18 │\n",
      "│ gemma-2-27b                         │ gemma-scope-27b-pt-res-canonical                    │ google/gemma-scope-27b-pt-res                          │        3 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-res                               │ google/gemma-scope-2b-pt-res                           │      310 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-res-canonical                     │ google/gemma-scope-2b-pt-res                           │       58 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-mlp                               │ google/gemma-scope-2b-pt-mlp                           │      260 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-mlp-canonical                     │ google/gemma-scope-2b-pt-mlp                           │       52 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-att                               │ google/gemma-scope-2b-pt-att                           │      260 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-att-canonical                     │ google/gemma-scope-2b-pt-att                           │       52 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_0824 │ canrager/lm_sae                                        │      180 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_0824 │ canrager/lm_sae                                        │      240 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     │ canrager/lm_sae                                        │      180 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     │ canrager/lm_sae                                        │      240 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-res                               │ google/gemma-scope-9b-pt-res                           │      562 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-res-canonical                     │ google/gemma-scope-9b-pt-res                           │       91 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-att                               │ google/gemma-scope-9b-pt-att                           │      492 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-att-canonical                     │ google/gemma-scope-9b-pt-att                           │       84 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-mlp                               │ google/gemma-scope-9b-pt-mlp                           │      492 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-mlp-canonical                     │ google/gemma-scope-9b-pt-mlp                           │       84 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-it-res                               │ google/gemma-scope-9b-it-res                           │       30 │\n",
      "│ gemma-2-9b-it                       │ gemma-scope-9b-it-res-canonical                     │ google/gemma-scope-9b-it-res                           │        6 │\n",
      "│ gemma-2b                            │ gemma-2b-res-jb                                     │ jbloom/Gemma-2b-Residual-Stream-SAEs                   │        5 │\n",
      "│ gemma-2b-it                         │ gemma-2b-it-res-jb                                  │ jbloom/Gemma-2b-IT-Residual-Stream-SAEs                │        1 │\n",
      "│ gpt2-small                          │ gpt2-small-res-jb                                   │ jbloom/GPT2-Small-SAEs-Reformatted                     │       13 │\n",
      "│ gpt2-small                          │ gpt2-small-hook-z-kk                                │ ckkissane/attn-saes-gpt2-small-all-layers              │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-tm                                   │ tommmcgrath/gpt2-small-mlp-out-saes                    │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-res-jb-feature-splitting                 │ jbloom/GPT2-Small-Feature-Splitting-Experiment-Layer-8 │        8 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-post-v5-32k                        │ jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs           │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-post-v5-128k                       │ jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs          │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-mid-v5-32k                         │ jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs            │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-mid-v5-128k                        │ jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs           │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-out-v5-32k                           │ jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs              │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-out-v5-128k                          │ jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs             │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-attn-out-v5-32k                          │ jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs             │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-attn-out-v5-128k                         │ jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs            │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sll-ajt                              │ neuronpedia/gpt2-small__res_sll-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_slefr-ajt                            │ neuronpedia/gpt2-small__res_slefr-ajt                  │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_scl-ajt                              │ neuronpedia/gpt2-small__res_scl-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sle-ajt                              │ neuronpedia/gpt2-small__res_sle-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sce-ajt                              │ neuronpedia/gpt2-small__res_sce-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_scefr-ajt                            │ neuronpedia/gpt2-small__res_scefr-ajt                  │        3 │\n",
      "│ meta-llama/Llama-3.1-8B             │ llama_scope_lxa_32x                                 │ fnlp/Llama3_1-8B-Base-LXA-32x                          │       32 │\n",
      "│ meta-llama/Llama-3.1-8B             │ llama_scope_lxa_8x                                  │ fnlp/Llama3_1-8B-Base-LXA-8x                           │       32 │\n",
      "│ meta-llama/Llama-3.1-8B             │ llama_scope_lxm_32x                                 │ fnlp/Llama3_1-8B-Base-LXM-32x                          │       32 │\n",
      "│ meta-llama/Llama-3.1-8B             │ llama_scope_lxm_8x                                  │ fnlp/Llama3_1-8B-Base-LXM-8x                           │       32 │\n",
      "│ meta-llama/Llama-3.1-8B             │ llama_scope_lxr_32x                                 │ fnlp/Llama3_1-8B-Base-LXR-32x                          │       32 │\n",
      "│ meta-llama/Llama-3.1-8B             │ llama_scope_lxr_8x                                  │ fnlp/Llama3_1-8B-Base-LXR-8x                           │       32 │\n",
      "│ meta-llama/Meta-Llama-3-8B-Instruct │ llama-3-8b-it-res-jh                                │ Juliushanhanhan/llama-3-8b-it-res                      │        1 │\n",
      "│ mistral-7b                          │ mistral-7b-res-wg                                   │ JoshEngels/Mistral-7B-Residual-Stream-SAEs             │        3 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-res-sm                           │ ctigges/pythia-70m-deduped__res-sm_processed           │        7 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-mlp-sm                           │ ctigges/pythia-70m-deduped__mlp-sm_processed           │        6 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-att-sm                           │ ctigges/pythia-70m-deduped__att-sm_processed           │        6 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_gated_ctx128_0730         │ canrager/lm_sae                                        │       40 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_panneal_ctx128_0730       │ canrager/lm_sae                                        │       56 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_standard_ctx128_0712      │ canrager/lm_sae                                        │       44 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_topk_ctx128_0730          │ canrager/lm_sae                                        │       48 │\n",
      "└─────────────────────────────────────┴─────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┴──────────┘\n",
      "┌────────────────────────┬─────────────────────────────────────────────────────────────────────────┐\n",
      "│ Field                  │ Value                                                                   │\n",
      "├────────────────────────┼─────────────────────────────────────────────────────────────────────────┤\n",
      "│ release                │ 'gpt2-small-res-jb'                                                     │\n",
      "│ repo_id                │ 'jbloom/GPT2-Small-SAEs-Reformatted'                                    │\n",
      "│ model                  │ 'gpt2-small'                                                            │\n",
      "│ conversion_func        │ None                                                                    │\n",
      "│ saes_map               │ {'blocks.0.hook_resid_pre': 'blocks.0.hook_resid_pre', ...}             │\n",
      "│ expected_var_explained │ {'blocks.0.hook_resid_pre': 0.999, ...}                                 │\n",
      "│ expected_l0            │ {'blocks.0.hook_resid_pre': 10.0, ...}                                  │\n",
      "│ neuronpedia_id         │ {'blocks.0.hook_resid_pre': 'gpt2-small/0-res-jb', ...}                 │\n",
      "│ config_overrides       │ {'model_from_pretrained_kwargs': {'center_writing_weights': True}, ...} │\n",
      "└────────────────────────┴─────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from sae_bench_utils.sae_selection_utils import get_saes_from_regex, print_all_sae_releases, print_release_details\n",
    "\n",
    "# Callum came up with this format which I like visually.\n",
    "print_all_sae_releases()\n",
    "print_release_details('gpt2-small-res-jb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cases:\n",
    "- Select all canonical Gemma Scope 2b res SAEs for all sizes, layer 12\n",
    "- Select all canonical Gemma Scope 2b, For layers 5,12,19, get all res, mlp and attn saes of size 16k or 65k\n",
    "- Select all Gemma Scope 2b, 16k res SAEs of all sparsities. \n",
    "- Select all sae bench gemma 2 2b SAEs vanilla, and topk, size 4k and 8k (both expansion factors, all sparsities)\n",
    "- Select all layer 3 and 4 pythia 70m SAES, Vanilla, TopK, Gated, P-Anneal, all sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b706b194dd44b55aea631b7363aca44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_bench_pythia70m_sweep_topk_ctx128_0730 2\n",
      "['blocks.4.hook_resid_post__trainer_2', 'blocks.4.hook_resid_post__trainer_10']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all canonical Gemma Scope 2b res SAEs for all sizes, layer 12\n",
    "sae_regex_pattern = r\"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_block_pattern = r\".*layer_12.*\"\n",
    "\n",
    "# all mlp Gemma Scope 2b SAEs for 16k size, layer 12\n",
    "sae_regex_pattern = r\"gemma-scope-2b-pt-mlp\"\n",
    "sae_block_pattern = r\".*layer_5.*(16k).*\"\n",
    "\n",
    "# canonical Gemma Scope 2b, For layers 5,12,19, get all res, mlp and attn saes of size 16k or 65k\n",
    "sae_regex_pattern = r\"(gemma-scope-2b-pt-(res|att|mlp)-canonical)\"\n",
    "sae_block_pattern = r\".*layer_(5|12|19).*(16k|65k).*\"\n",
    "\n",
    "# Select all sae bench layer 19 gemma-2-2b topk 16k width SAEs (excluding checkpoints)\n",
    "sae_regex_pattern = r\"sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824\"\n",
    "sae_block_pattern = r\".*blocks\\.19(?!.*step).*\"\n",
    "\n",
    "# Select all sae bench layer 19 gemma-2-2b topk 16k width SAEs (including checkpoints)\n",
    "sae_regex_pattern = r\"sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824\"\n",
    "sae_block_pattern = r\".*blocks\\.19.*\"\n",
    "\n",
    "# Select all sae bench layer 3 and 4 pythia 70M SAEs: Vanilla, TopK, Gated, P-Anneal, all sparsities\n",
    "sae_regex_pattern = r\"sae_bench_pythia70m_sweep.*_ctx128_.*\"\n",
    "sae_block_pattern = r\".*blocks\\.([3|4])\\.hook_resid_post__trainer_.*\"\n",
    "\n",
    "# Select trainer ids 2 and 10 for layer 4 pythia 70m topk SAEs\n",
    "sae_regex_pattern = r\"(sae_bench_pythia70m_sweep_topk_ctx128_0730).*\"\n",
    "sae_block_pattern = r\".*blocks\\.([4])\\.hook_resid_post__trainer_(2|10)$\"\n",
    "\n",
    "selected_saes = get_saes_from_regex(sae_regex_pattern, sae_block_pattern)\n",
    "\n",
    "print(f\"Selected {len(selected_saes)} SAEs:\")\n",
    "\n",
    "for sae_release, sae_id in selected_saes:\n",
    "    print(f\"{sae_release} - {sae_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae_bench_template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
